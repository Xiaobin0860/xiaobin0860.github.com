---
layout: post
title: "AI数学基础-线性代数(1)"
date: 2021-08-07
categories: AI
---

## 人工智能理解

建立在以线性代数和概率论为骨架的基础数学上，通过简单的模型组合实现复杂的功能。

## 线性代数的核心意义

万事万物都可以被抽象成某些特征的组合，并在由预置规则定义的框架之下以静态和动态的方式加以观察。

用虚拟数字世界表示真实物理世界的工具。

## 线性代数基本概念

1. 集合(set)：某些特定对象汇总成的集体
2. 标量(scalar)：单独数`a`构成的元素，`a`可以是`整数`、`实数`或`复数`
3. 向量(vector)：多个标量按一定顺序组成一个序列

   向量可以看作标量的扩展，原始的数被替代为一组数，从而带来了维度的增加，给定表示索引的下标才能唯一确定向量中的元素

4. 矩阵(matrix)：交向量的所有标量都替换成同规格的向量

   相对于向量，矩阵同样代表了维度的增加，矩阵中的每个元素需要两个索引确定

5. 张量(tensor)：将矩阵中的每个标量再替换为向量，得到的就是张量。张量就是高阶矩阵

## 描述向量的数学语言

1. 范数(norm)：对单个向量大小的度量，描述的是向量自身的性质，其作用是将向量映射为一个非负的数值。通用$L^p 范数定义如下：

   $∣\vec x∣_p​ = (\sum_i∣x_i​∣^p)^\frac{1}{p}$​

   对于一个给定的向量，$L^1$范数计算的是向量所有元素绝对值的和，$L^2$范数计算的是通常意义的向量长度，$L^\infty$范数计算的则是向量中最大元素的取值。

2. 内积(inner product)：计算的是两个向量的关系。两个同维向量内积的表达式为

   $<\vec x, \vec y> = \sum_ix_i*y_i$

   即对应元素乘积的求和。内积能表示两个向量的相对位置，即向量之间的夹角。一种特殊的情况是内积是 0，即$<\vec x, \vec y> = 0$，在二维空间上，这意味着两个向量夹角 90 度，即相互垂直。而在高维空间上，这咱关系被称为`正交(orthogonality)`。如果两个向量正交，说明它们线性无关，相互独立，互不影响。

---
